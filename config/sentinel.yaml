# LLM-Sentinel Configuration
# This config matches the expected schema from sentinel-core/config.rs

# Server configuration
server:
  host: "0.0.0.0"
  port: 8080
  worker_threads: 4
  request_timeout_secs: 30
  shutdown_timeout_secs: 10

# Ingestion configuration
ingestion:
  buffer_size: 10000
  batch_size: 100
  batch_timeout_ms: 1000

# Detection configuration
detection:
  engines:
    - engine_type: "statistical"
      methods:
        - "zscore"
        - "iqr"
        - "mad"
        - "cusum"
      settings: {}
  workers: 4
  timeout_ms: 5000
  enable_ml: false
  model_update_interval_secs: 3600

# Alerting configuration
alerting:
  dedup_window_secs: 300
  batch_size: 100
  batch_timeout_ms: 5000

# Storage configuration
storage:
  influxdb:
    url: "https://us-central1-1.gcp.cloud2.influxdata.com"
    org: "agentics"
    bucket: "sentinel"
    token: "placeholder-will-be-overridden-by-env"
    timeout_secs: 30
  cache:
    cache_type: "moka"
    max_capacity: 10000
    ttl_secs: 300

# Observability configuration
observability:
  enable_metrics: true
  metrics_port: 9090
  enable_tracing: false
  log_level: "info"
  log_format: "json"
